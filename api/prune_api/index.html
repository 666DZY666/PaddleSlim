<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>剪枝与敏感度 - PaddleSlim Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u526a\u679d\u4e0e\u654f\u611f\u5ea6";
    var mkdocs_page_input_path = "api/prune_api.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> PaddleSlim Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../model_zoo/">模型库</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">教程</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../tutorials/quant_post_demo/">离线量化</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tutorials/quant_aware_demo/">量化训练</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tutorials/quant_embedding_demo/">Embedding量化</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tutorials/nas_demo/">SA搜索</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tutorials/distillation_demo/">知识蒸馏</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../quantization_api/">量化</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">剪枝与敏感度</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#pruner">Pruner</a></li>
    

    <li class="toctree-l3"><a href="#sensitivity">sensitivity</a></li>
    

    <li class="toctree-l3"><a href="#merge_sensitive">merge_sensitive</a></li>
    

    <li class="toctree-l3"><a href="#load_sensitivities">load_sensitivities</a></li>
    

    <li class="toctree-l3"><a href="#get_ratios_by_loss">get_ratios_by_loss</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../analysis_api/">模型分析</a>
                </li>
                <li class="">
                    
    <a class="" href="../single_distiller_api/">知识蒸馏</a>
                </li>
                <li class="">
                    
    <a class="" href="../nas_api/">SA搜索</a>
                </li>
                <li class="">
                    
    <a class="" href="../../search_space/">搜索空间</a>
                </li>
                <li class="">
                    
    <a class="" href="../../table_latency/">硬件延时评估表</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../algo/algo/">算法原理</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">PaddleSlim Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>API &raquo;</li>
        
      
    
    <li>剪枝与敏感度</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/PaddlePaddle/PaddleSlim/edit/master/docs/api/prune_api.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="pruner">Pruner<a class="headerlink" href="#pruner" title="Permanent link">#</a></h2>
<dl>
<dt>paddleslim.prune.Pruner(criterion="l1_norm")<a href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/paddleslim/prune/pruner.py#L28">源代码</a></dt>
<dd>
<p>对卷积网络的通道进行一次剪裁。剪裁一个卷积层的通道，是指剪裁该卷积层输出的通道。卷积层的权重形状为<code>[output_channel, input_channel, kernel_size, kernel_size]</code>，通过剪裁该权重的第一纬度达到剪裁输出通道数的目的。</p>
</dd>
</dl>
<p><strong>参数：</strong></p>
<ul>
<li><strong>criterion</strong> - 评估一个卷积层内通道重要性所参考的指标。目前仅支持<code>l1_norm</code>。默认为<code>l1_norm</code>。</li>
</ul>
<p><strong>返回：</strong> 一个Pruner类的实例</p>
<p><strong>示例代码：</strong></p>
<div class="highlight"><pre><span></span>from paddleslim.prune import Pruner
pruner = Pruner()
</pre></div>

<dl>
<dt>paddleslim.prune.Pruner.prune(program, scope, params, ratios, place=None, lazy=False, only_graph=False, param_backup=False, param_shape_backup=False)<a href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/paddleslim/prune/pruner.py#L36">源代码</a></dt>
<dd>
<p>对目标网络的一组卷积层的权重进行裁剪。</p>
</dd>
</dl>
<p><strong>参数：</strong></p>
<ul>
<li>
<p><strong>program(paddle.fluid.Program)</strong> - 要裁剪的目标网络。更多关于Program的介绍请参考：<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/fluid_cn/Program_cn.html#program">Program概念介绍</a>。</p>
</li>
<li>
<p><strong>scope(paddle.fluid.Scope)</strong> - 要裁剪的权重所在的<code>scope</code>，Paddle中用<code>scope</code>实例存放模型参数和运行时变量的值。Scope中的参数值会被<code>inplace</code>的裁剪。更多介绍请参考<a href="">Scope概念介绍</a></p>
</li>
<li>
<p><strong>params(list<str>)</strong> - 需要被裁剪的卷积层的参数的名称列表。可以通过以下方式查看模型中所有参数的名称:
<div class="highlight"><pre><span></span>for block in program.blocks:
    for param in block.all_parameters():
        print(&quot;param: {}; shape: {}&quot;.format(param.name, param.shape))
</pre></div></p>
</li>
<li>
<p><strong>ratios(list<float>)</strong> - 用于裁剪<code>params</code>的剪切率，类型为列表。该列表长度必须与<code>params</code>的长度一致。</p>
</li>
<li>
<p><strong>place(paddle.fluid.Place)</strong> - 待裁剪参数所在的设备位置，可以是<code>CUDAPlace</code>或<code>CPUPlace</code>。<a href="">Place概念介绍</a></p>
</li>
<li>
<p><strong>lazy(bool)</strong> - <code>lazy</code>为True时，通过将指定通道的参数置零达到裁剪的目的，参数的<code>shape保持不变</code>；<code>lazy</code>为False时，直接将要裁的通道的参数删除，参数的<code>shape</code>会发生变化。</p>
</li>
<li>
<p><strong>only_graph(bool)</strong> - 是否只裁剪网络结构。在Paddle中，Program定义了网络结构，Scope存储参数的数值。一个Scope实例可以被多个Program使用，比如定义了训练网络的Program和定义了测试网络的Program是使用同一个Scope实例的。<code>only_graph</code>为True时，只对Program中定义的卷积的通道进行剪裁；<code>only_graph</code>为false时，Scope中卷积参数的数值也会被剪裁。默认为False。</p>
</li>
<li>
<p><strong>param_backup(bool)</strong> - 是否返回对参数值的备份。默认为False。</p>
</li>
<li>
<p><strong>param_shape_backup(bool)</strong> - 是否返回对参数<code>shape</code>的备份。默认为False。</p>
</li>
</ul>
<p><strong>返回：</strong></p>
<ul>
<li>
<p><strong>pruned_program(paddle.fluid.Program)</strong> - 被裁剪后的Program。</p>
</li>
<li>
<p><strong>param_backup(dict)</strong> - 对参数数值的备份，用于恢复Scope中的参数数值。</p>
</li>
<li>
<p><strong>param_shape_backup(dict)</strong> - 对参数形状的备份。</p>
</li>
</ul>
<p><strong>示例：</strong></p>
<p>点击<a href="https://aistudio.baidu.com/aistudio/projectDetail/200786">AIStudio</a>执行以下示例代码。
<div class="highlight"><pre><span></span>import paddle.fluid as fluid
from paddle.fluid.param_attr import ParamAttr
from paddleslim.prune import Pruner

def conv_bn_layer(input,
                  num_filters,
                  filter_size,
                  name,
                  stride=1,
                  groups=1,
                  act=None):
    conv = fluid.layers.conv2d(
        input=input,
        num_filters=num_filters,
        filter_size=filter_size,
        stride=stride,
        padding=(filter_size - 1) // 2,
        groups=groups,
        act=None,
        param_attr=ParamAttr(name=name + &quot;_weights&quot;),
        bias_attr=False,
        name=name + &quot;_out&quot;)
    bn_name = name + &quot;_bn&quot;
    return fluid.layers.batch_norm(
        input=conv,
        act=act,
        name=bn_name + &#39;_output&#39;,
        param_attr=ParamAttr(name=bn_name + &#39;_scale&#39;),
        bias_attr=ParamAttr(bn_name + &#39;_offset&#39;),
        moving_mean_name=bn_name + &#39;_mean&#39;,
        moving_variance_name=bn_name + &#39;_variance&#39;, )

main_program = fluid.Program()
startup_program = fluid.Program()
#   X       X              O       X              O
# conv1--&gt;conv2--&gt;sum1--&gt;conv3--&gt;conv4--&gt;sum2--&gt;conv5--&gt;conv6
#     |            ^ |                    ^
#     |____________| |____________________|
#
# X: prune output channels
# O: prune input channels
with fluid.program_guard(main_program, startup_program):
    input = fluid.data(name=&quot;image&quot;, shape=[None, 3, 16, 16])
    conv1 = conv_bn_layer(input, 8, 3, &quot;conv1&quot;)
    conv2 = conv_bn_layer(conv1, 8, 3, &quot;conv2&quot;)
    sum1 = conv1 + conv2
    conv3 = conv_bn_layer(sum1, 8, 3, &quot;conv3&quot;)
    conv4 = conv_bn_layer(conv3, 8, 3, &quot;conv4&quot;)
    sum2 = conv4 + sum1
    conv5 = conv_bn_layer(sum2, 8, 3, &quot;conv5&quot;)
    conv6 = conv_bn_layer(conv5, 8, 3, &quot;conv6&quot;)

place = fluid.CPUPlace()
exe = fluid.Executor(place)
scope = fluid.Scope()
exe.run(startup_program, scope=scope)
pruner = Pruner()
main_program, _, _ = pruner.prune(
    main_program,
    scope,
    params=[&quot;conv4_weights&quot;],
    ratios=[0.5],
    place=place,
    lazy=False,
    only_graph=False,
    param_backup=False,
    param_shape_backup=False)

for param in main_program.global_block().all_parameters():
    if &quot;weights&quot; in param.name:
        print(&quot;param name: {}; param shape: {}&quot;.format(param.name, param.shape))
</pre></div></p>
<hr />
<h2 id="sensitivity">sensitivity<a class="headerlink" href="#sensitivity" title="Permanent link">#</a></h2>
<dl>
<dt>paddleslim.prune.sensitivity(program, place, param_names, eval_func, sensitivities_file=None, pruned_ratios=None) <a href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/paddleslim/prune/sensitive.py#L34">源代码</a></dt>
<dd>
<p>计算网络中每个卷积层的敏感度。每个卷积层的敏感度信息统计方法为：依次剪掉当前卷积层不同比例的输出通道数，在测试集上计算剪裁后的精度损失。得到敏感度信息后，可以通过观察或其它方式确定每层卷积的剪裁率。</p>
</dd>
</dl>
<p><strong>参数：</strong></p>
<ul>
<li>
<p><strong>program(paddle.fluid.Program)</strong> - 待评估的目标网络。更多关于Program的介绍请参考：<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/fluid_cn/Program_cn.html#program">Program概念介绍</a>。</p>
</li>
<li>
<p><strong>place(paddle.fluid.Place)</strong> - 待分析的参数所在的设备位置，可以是<code>CUDAPlace</code>或<code>CPUPlace</code>。<a href="">Place概念介绍</a></p>
</li>
<li>
<p><strong>param_names(list<str>)</strong> - 待分析的卷积层的参数的名称列表。可以通过以下方式查看模型中所有参数的名称:</p>
</li>
</ul>
<div class="highlight"><pre><span></span>for block in program.blocks:
    for param in block.all_parameters():
        print(&quot;param: {}; shape: {}&quot;.format(param.name, param.shape))
</pre></div>

<ul>
<li>
<p><strong>eval_func(function)</strong> - 用于评估裁剪后模型效果的回调函数。该回调函数接受被裁剪后的<code>program</code>为参数，返回一个表示当前program的精度，用以计算当前裁剪带来的精度损失。</p>
</li>
<li>
<p><strong>sensitivities_file(str)</strong> - 保存敏感度信息的本地文件系统的文件。在敏感度计算过程中，会持续将新计算出的敏感度信息追加到该文件中。重启任务后，文件中已有敏感度信息不会被重复计算。该文件可以用<code>pickle</code>加载。</p>
</li>
<li>
<p><strong>pruned_ratios(list<float>)</strong> - 计算卷积层敏感度信息时，依次剪掉的通道数比例。默认为[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]。</p>
</li>
</ul>
<p><strong>返回：</strong></p>
<ul>
<li><strong>sensitivities(dict)</strong> - 存放敏感度信息的dict，其格式为：</li>
</ul>
<div class="highlight"><pre><span></span>{&quot;weight_0&quot;:
   {0.1: 0.22,
    0.2: 0.33
   },
 &quot;weight_1&quot;:
   {0.1: 0.21,
    0.2: 0.4
   }
}
</pre></div>

<p>其中，<code>weight_0</code>是卷积层参数的名称，sensitivities['weight_0']的<code>value</code>为剪裁比例，<code>value</code>为精度损失的比例。</p>
<p><strong>示例：</strong></p>
<p>点击<a href="https://aistudio.baidu.com/aistudio/projectdetail/201401">AIStudio</a>运行以下示例代码。</p>
<div class="highlight"><pre><span></span>import paddle
import numpy as np
import paddle.fluid as fluid
from paddle.fluid.param_attr import ParamAttr
from paddleslim.prune import sensitivity
import paddle.dataset.mnist as reader

def conv_bn_layer(input,
                  num_filters,
                  filter_size,
                  name,
                  stride=1,
                  groups=1,
                  act=None):
    conv = fluid.layers.conv2d(
        input=input,
        num_filters=num_filters,
        filter_size=filter_size,
        stride=stride,
        padding=(filter_size - 1) // 2,
        groups=groups,
        act=None,
        param_attr=ParamAttr(name=name + &quot;_weights&quot;),
        bias_attr=False,
        name=name + &quot;_out&quot;)
    bn_name = name + &quot;_bn&quot;
    return fluid.layers.batch_norm(
        input=conv,
        act=act,
        name=bn_name + &#39;_output&#39;,
        param_attr=ParamAttr(name=bn_name + &#39;_scale&#39;),
        bias_attr=ParamAttr(bn_name + &#39;_offset&#39;),
        moving_mean_name=bn_name + &#39;_mean&#39;,
        moving_variance_name=bn_name + &#39;_variance&#39;, )

main_program = fluid.Program()
startup_program = fluid.Program()
#   X       X              O       X              O
# conv1--&gt;conv2--&gt;sum1--&gt;conv3--&gt;conv4--&gt;sum2--&gt;conv5--&gt;conv6
#     |            ^ |                    ^
#     |____________| |____________________|
#
# X: prune output channels
# O: prune input channels
image_shape = [1,28,28]
with fluid.program_guard(main_program, startup_program):
    image = fluid.data(name=&#39;image&#39;, shape=[None]+image_shape, dtype=&#39;float32&#39;)
    label = fluid.data(name=&#39;label&#39;, shape=[None, 1], dtype=&#39;int64&#39;)  
    conv1 = conv_bn_layer(image, 8, 3, &quot;conv1&quot;)
    conv2 = conv_bn_layer(conv1, 8, 3, &quot;conv2&quot;)
    sum1 = conv1 + conv2
    conv3 = conv_bn_layer(sum1, 8, 3, &quot;conv3&quot;)
    conv4 = conv_bn_layer(conv3, 8, 3, &quot;conv4&quot;)
    sum2 = conv4 + sum1
    conv5 = conv_bn_layer(sum2, 8, 3, &quot;conv5&quot;)
    conv6 = conv_bn_layer(conv5, 8, 3, &quot;conv6&quot;)
    out = fluid.layers.fc(conv6, size=10, act=&quot;softmax&quot;)
#    cost = fluid.layers.cross_entropy(input=out, label=label)
#    avg_cost = fluid.layers.mean(x=cost)
    acc_top1 = fluid.layers.accuracy(input=out, label=label, k=1)
#    acc_top5 = fluid.layers.accuracy(input=out, label=label, k=5)


place = fluid.CPUPlace()
exe = fluid.Executor(place)
exe.run(startup_program)

val_reader = paddle.batch(reader.test(), batch_size=128)
val_feeder = feeder = fluid.DataFeeder(
        [image, label], place, program=main_program)

def eval_func(program):

    acc_top1_ns = []
    for data in val_reader():
        acc_top1_n = exe.run(program,
                             feed=val_feeder.feed(data),
                             fetch_list=[acc_top1.name])
        acc_top1_ns.append(np.mean(acc_top1_n))
    return np.mean(acc_top1_ns)
param_names = []
for param in main_program.global_block().all_parameters():
    if &quot;weights&quot; in param.name:
        param_names.append(param.name)
sensitivities = sensitivity(main_program,
                            place,
                            param_names,
                            eval_func,
                            sensitivities_file=&quot;./sensitive.data&quot;,
                            pruned_ratios=[0.1, 0.2, 0.3])
print(sensitivities)
</pre></div>

<h2 id="merge_sensitive">merge_sensitive<a class="headerlink" href="#merge_sensitive" title="Permanent link">#</a></h2>
<dl>
<dt>paddleslim.prune.merge_sensitive(sensitivities)<a href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/paddleslim/prune/sensitive.py#L161">源代码</a></dt>
<dd>
<p>合并多个敏感度信息。</p>
</dd>
</dl>
<p>参数：</p>
<ul>
<li><strong>sensitivities(list<dict> | list<str>)</strong> - 待合并的敏感度信息，可以是字典的列表，或者是存放敏感度信息的文件的路径列表。</li>
</ul>
<p>返回：</p>
<ul>
<li><strong>sensitivities(dict)</strong> - 合并后的敏感度信息。其格式为：</li>
</ul>
<div class="highlight"><pre><span></span>{&quot;weight_0&quot;:
   {0.1: 0.22,
    0.2: 0.33
   },
 &quot;weight_1&quot;:
   {0.1: 0.21,
    0.2: 0.4
   }
}
</pre></div>

<p>其中，<code>weight_0</code>是卷积层参数的名称，sensitivities['weight_0']的<code>value</code>为剪裁比例，<code>value</code>为精度损失的比例。</p>
<p>示例：</p>
<h2 id="load_sensitivities">load_sensitivities<a class="headerlink" href="#load_sensitivities" title="Permanent link">#</a></h2>
<dl>
<dt>paddleslim.prune.load_sensitivities(sensitivities_file)<a href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/paddleslim/prune/sensitive.py#L184">源代码</a></dt>
<dd>
<p>从文件中加载敏感度信息。</p>
</dd>
</dl>
<p>参数：</p>
<ul>
<li><strong>sensitivities_file(str)</strong> - 存放敏感度信息的本地文件.</li>
</ul>
<p>返回：</p>
<ul>
<li><strong>sensitivities(dict)</strong> - 敏感度信息。</li>
</ul>
<p>示例：</p>
<h2 id="get_ratios_by_loss">get_ratios_by_loss<a class="headerlink" href="#get_ratios_by_loss" title="Permanent link">#</a></h2>
<dl>
<dt>paddleslim.prune.get_ratios_by_loss(sensitivities, loss)<a href="https://github.com/PaddlePaddle/PaddleSlim/blob/develop/paddleslim/prune/sensitive.py#L206">源代码</a></dt>
<dd>
<p>根据敏感度和精度损失阈值计算出一组剪切率。对于参数<code>w</code>, 其剪裁率为使精度损失低于<code>loss</code>的最大剪裁率。</p>
</dd>
</dl>
<p>参数：</p>
<ul>
<li>
<p><strong>sensitivities(dict)</strong> - 敏感度信息。</p>
</li>
<li>
<p><strong>loss</strong> - 精度损失阈值。</p>
</li>
</ul>
<p>返回：</p>
<ul>
<li><strong>ratios(dict)</strong> - 一组剪切率。<code>key</code>是待剪裁参数的名称。<code>value</code>是对应参数的剪裁率。</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../analysis_api/" class="btn btn-neutral float-right" title="模型分析">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../quantization_api/" class="btn btn-neutral" title="量化"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/PaddlePaddle/PaddleSlim/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../quantization_api/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../analysis_api/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../mathjax-config.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
